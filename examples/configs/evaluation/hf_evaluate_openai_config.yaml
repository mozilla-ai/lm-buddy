name: "lm-buddy-hf-evaluate-oai"

# Input dataset path
dataset:
  path: "s3://platform-storage/datasets/dialogsum"

# Settings specific to the hf_evaluate entrypoint
evaluation:
  # metrics to be used for the evaluation
  # (you can add "rouge", "meteor", and "bertscore" atm)
  metrics: ["rouge", "meteor", "bertscore"]
  # enable/disable tqdm to track eval progress
  # (useful when running interactively, noisy on ray logs)
  enable_tqdm: True
  # rely on HF pipeline for summarization (ignored if using OAI API)
  use_pipeline: True
  # perform inference / evaluation on the first max_samples only
  max_samples: 10
  # output file path
  # - if you provide a path complete with a filename, results will be stored in it
  # - if you provide a dir, results will be stored in <dir>/<config.name>/eval_results.json
  # - if you don't provide a storage path, results will be stored locally (see ~/.lm-buddy/results)
  # storage_path: "s3://platform-storage/experiments/results/"

# Model to evaluate (OpenAI)
# - The base_url is fixed
# - Choose an engine name (see https://platform.openai.com/docs/models)
# - Customize the system prompt if needed
model:
  inference:
    base_url: "https://api.openai.com/v1"
    engine: "oai://gpt-4-turbo"
    system_prompt: "You are a helpful assistant, expert in text summarization. For every prompt you receive, provide a summary of its contents in at most two sentences."
    max_retries: 3
