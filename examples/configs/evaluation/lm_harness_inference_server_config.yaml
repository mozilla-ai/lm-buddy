# Model to evaluate specified as a local-chat-completions inference server
model:
  inference:
    base_url: "http://1.2.3.4:8000/v1"
    # HuggingFace repo for the engine model being hosted
    engine:
      repo_id: "distilgpt2"
      revision: "main"
    # # W&B artifact can also be specified as the engine model to generate a lineage
    # engine:
    #   name: "wandb-artifact-name"
    #   project: "flamingo-examples"
    #   entity: "mozilla-ai"
  # 'huggingface' or 'tiktoken' depending on model type
  tokenizer_backend: "huggingface"

# Settings specific to lm_harness.evaluate
evaluator:
  tasks: ["gsm8k"]
  num_fewshot: 5
  limit: 10

tracking:
  name: "flamingo-example-lm-harness-inference"
  project: "flamingo-examples"
  entity: "mozilla-ai"
